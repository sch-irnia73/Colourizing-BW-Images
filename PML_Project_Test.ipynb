{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from skimage import color\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import sys\n",
    "import argparse\n",
    "import shutil\n",
    "import collections\n",
    "import numpy as np\n",
    "import scipy.misc as misc\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "   'path': '',\n",
    "   'dataset': '',\n",
    "   'large': False,\n",
    "   'batch_size': 32,\n",
    "   'lr': 1e-4,\n",
    "   'weight_decay': 0,\n",
    "   'num_epoch': 15,\n",
    "   'lamb': 100,\n",
    "   'test': '',\n",
    "   'generator': 'model/1126.filter1/GAN__100L1_bs32_Adam_lr0.0001/G_epoch5.pth.tar',\n",
    "   'generator_large': 'model/1127.large/GAN__100L1_bs32_Adam_lr0.0001/G_epoch14.pth.tar',\n",
    "   'discriminator': 'model/1126.filter1/GAN__100L1_bs32_Adam_lr0.0001/D_epoch5.pth.tar',\n",
    "   'discriminator_large': 'model/1127.large/GAN__100L1_bs32_Adam_lr0.0001/D_epoch14.pth.tar',\n",
    "   'save': True,\n",
    "   'gpu': 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 128, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 128, 3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.relu4 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.deconv5 = nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        self.relu5 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.deconv6 = nn.ConvTranspose2d(64, 3, 3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(3)\n",
    "        self.relu6 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = self.conv1(h)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu1(h) # 64, 16, 16\n",
    "        pool1 = h\n",
    "        \n",
    "        h = self.conv2(h)\n",
    "        h = self.bn2(h)\n",
    "        h = self.relu2(h) # 128, 8, 8\n",
    "        pool2 = h\n",
    "        \n",
    "        h = self.conv3(h)\n",
    "        h = self.bn3(h)\n",
    "        h = self.relu3(h) # 128, 4, 4\n",
    "        \n",
    "        h = self.deconv4(h)\n",
    "        h = self.bn4(h)\n",
    "        h = self.relu4(h) # 128, 8, 8\n",
    "        h += pool2\n",
    "        \n",
    "        h = self.deconv5(h)\n",
    "        h = self.bn5(h)\n",
    "        h = self.relu5(h) # 64, 16, 16\n",
    "        h += pool1\n",
    "        \n",
    "        h = self.deconv6(h)\n",
    "        h = F.tanh(h) # 3, 32, 32\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 128, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 128, 4, stride=1, padding=0, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.relu4 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128, 1, 1, stride=1, padding=0, bias=False)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = self.conv1(h)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu1(h) # 64, 16, 16\n",
    "        \n",
    "        h = self.conv2(h)\n",
    "        h = self.bn2(h)\n",
    "        h = self.relu2(h) # 128, 8, 8\n",
    "        \n",
    "        h = self.conv3(h)\n",
    "        h = self.bn3(h)\n",
    "        h = self.relu3(h) # 128, 4, 4\n",
    "        \n",
    "        h = self.conv4(h)\n",
    "        h = self.bn4(h)\n",
    "        h = self.relu4(h) # 128, 1, 1\n",
    "        \n",
    "        h = self.conv5(h)\n",
    "        h = F.sigmoid(h)\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator_Large(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.relu3 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(256, 256, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.relu4 = nn.LeakyReLU(0.1)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(256, 256, 26, stride=2, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.relu5 = nn.LeakyReLU(0.1)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(256, 1, 1, stride=1, padding=0, bias=False)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = self.conv1(h)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu1(h) # 64, 200, 200\n",
    "        \n",
    "        h = self.conv2(h)\n",
    "        h = self.bn2(h)\n",
    "        h = self.relu2(h) # 128, 100, 100\n",
    "        \n",
    "        h = self.conv3(h)\n",
    "        h = self.bn3(h)\n",
    "        h = self.relu3(h) # 256, 50, 50\n",
    "        \n",
    "        h = self.conv4(h)\n",
    "        h = self.bn4(h)\n",
    "        h = self.relu4(h) # 256, 25, 25\n",
    "\n",
    "        h = self.conv5(h)\n",
    "        h = self.bn5(h)\n",
    "        h = self.relu5(h) # 256, 1, 1\n",
    "        \n",
    "        h = self.conv6(h)\n",
    "        h = F.sigmoid(h)\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "\n",
    "class Generator_Large(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.LeakyReLU(0.1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.relu3 = nn.LeakyReLU(0.1)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(256, 256, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.relu4 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.deconv5 = nn.ConvTranspose2d(256, 256, 3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.relu5 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.deconv6 = nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        self.relu6 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.deconv7 = nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "        self.bn7 = nn.BatchNorm2d(64)\n",
    "        self.relu7 = nn.LeakyReLU(0.1)\n",
    "\n",
    "        self.deconv8 = nn.ConvTranspose2d(64, 3, 3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "        self.bn8 = nn.BatchNorm2d(3)\n",
    "        self.relu8 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = self.conv1(h)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu1(h) # 64, 200, 200\n",
    "        pool1 = h\n",
    "        \n",
    "        h = self.conv2(h)\n",
    "        h = self.bn2(h)\n",
    "        h = self.relu2(h) # 128, 100, 100\n",
    "        pool2 = h\n",
    "        \n",
    "        h = self.conv3(h)\n",
    "        h = self.bn3(h)\n",
    "        h = self.relu3(h) # 256, 50, 50\n",
    "        pool3 = h\n",
    "        \n",
    "        h = self.conv4(h)\n",
    "        h = self.bn4(h)\n",
    "        h = self.relu4(h) # 256, 25, 25\n",
    "\n",
    "        h = self.deconv5(h)\n",
    "        h = self.bn5(h)\n",
    "        h = self.relu5(h) # 256, 50, 50\n",
    "        h += pool3\n",
    "\n",
    "        h = self.deconv6(h)\n",
    "        h = self.bn6(h)\n",
    "        h = self.relu6(h) # 128, 100, 100\n",
    "        h += pool2\n",
    "\n",
    "        h = self.deconv7(h)\n",
    "        h = self.bn7(h)\n",
    "        h = self.relu7(h) # 64, 200, 200\n",
    "        h += pool1\n",
    "        \n",
    "        h = self.deconv8(h)\n",
    "        h = F.tanh(h) # 3, 400, 400\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args_dict['large']:\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.CenterCrop(400), transforms.ToTensor()]\n",
    "    )\n",
    "else:\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.CenterCrop(32), transforms.ToTensor()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "   def __init__(self, X_dir, transform=None):\n",
    "       self.X_dir = X_dir\n",
    "       self.transform = transform\n",
    "       self.X_filenames = os.listdir(X_dir)\n",
    "\n",
    "   def __len__(self):\n",
    "       return len(self.X_filenames)\n",
    "\n",
    "   def __getitem__(self, idx):\n",
    "       X_path = os.path.join(self.X_dir, self.X_filenames[idx])\n",
    "       X = Image.open(X_path).convert('L')\n",
    "       if self.transform:\n",
    "           X = self.transform(X)\n",
    "       return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = 'data/vis' if args_dict['large'] else 'dataset/vis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = CustomDataset(X_test, transform=transform)\n",
    "\n",
    "train_loader = data.DataLoader(dataset_train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args_dict['large']:\n",
    "    generator = Generator_Large().to(device=device)\n",
    "    discriminator = Discriminator_Large().to(device=device)\n",
    "else:\n",
    "    generator = Generator().to(device=device)\n",
    "    discriminator = Discriminator().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume model G: model/1126.filter1/GAN__100L1_bs32_Adam_lr0.0001/G_epoch5.pth.tar\n",
      "Resume model D: model/1126.filter1/GAN__100L1_bs32_Adam_lr0.0001/D_epoch5.pth.tar\n"
     ]
    }
   ],
   "source": [
    "if args_dict['large']:\n",
    "    G = args_dict['generator_large']\n",
    "    D = args_dict['discriminator_large']\n",
    "else:\n",
    "    G = args_dict['generator']\n",
    "    D = args_dict['discriminator']\n",
    "start_epoch_G = start_epoch_D = 0\n",
    "if G:\n",
    "    print('Resume model G: %s' % G)\n",
    "    checkpoint_G = torch.load(G)\n",
    "    generator.load_state_dict(checkpoint_G['state_dict'])\n",
    "    start_epoch_G = checkpoint_G['epoch']\n",
    "if D:\n",
    "    print('Resume model D: %s' % D)\n",
    "    checkpoint_D = torch.load(D)\n",
    "    discriminator.load_state_dict(checkpoint_D['state_dict'])\n",
    "    start_epoch_D = checkpoint_D['epoch']\n",
    "#assert start_epoch_G == start_epoch_D\n",
    "if G == '' and D == '':\n",
    "    print('No Resume')\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "num_epochs = 50\n",
    "criterion = nn.BCELoss()\n",
    "L1 = nn.L1Loss()\n",
    "\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), \n",
    "            lr=args_dict['lr'], betas=(0.5, 0.999), \n",
    "            eps=1e-8, weight_decay=args_dict['weight_decay'])\n",
    "optimizer_generator = optim.Adam(generator.parameters(), \n",
    "            lr=args_dict['lr'], betas=(0.5, 0.999),\n",
    "            eps=1e-8, weight_decay=args_dict['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = 'Test'\n",
    "size = ''\n",
    "img_path = 'img/%s/GAN_%s%s_%dL1_bs%d_%s_lr%s/' \\\n",
    "           % (date, args_dict['dataset'], size, args_dict['lamb'], args_dict['batch_size'], 'Adam', str(args_dict['lr']))\n",
    "model_path = 'model/%s/GAN_%s%s_%dL1_bs%d_%s_lr%s/' \\\n",
    "           % (date, args_dict['dataset'], size, args_dict['lamb'], args_dict['batch_size'], 'Adam', str(args_dict['lr']))\n",
    "if not os.path.exists(img_path):\n",
    "    os.makedirs(img_path)\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data) in enumerate(train_loader):\n",
    "    latent_space_samples = Variable(data.to(device=device))\n",
    "\n",
    "\n",
    "generated_samples = generator(latent_space_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 3600x2700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3600x2700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3600x2700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_samples = generated_samples.cpu().detach()\n",
    "\n",
    "\n",
    "for i in range(0, len(generated_samples)):\n",
    "    l = torch.unsqueeze(torch.squeeze(latent_space_samples[i]), 0).cpu().numpy()\n",
    "    pred = generated_samples[i].cpu().numpy()\n",
    "\n",
    "    pred_rgb = (np.transpose(pred, (1,2,0)).astype(np.float64) + 1) / 2.\n",
    "\n",
    "    grey = np.transpose(l, (1,2,0))\n",
    "    grey = np.repeat(grey, 3, axis=2).astype(np.float64)\n",
    "    img_list = np.concatenate((grey, pred_rgb), 1)\n",
    "\n",
    "    plt.figure(figsize=(36,27))\n",
    "    plt.imshow(img_list)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_path + f'test{i}.png')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
